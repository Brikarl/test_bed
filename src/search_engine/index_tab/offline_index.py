#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ç¦»çº¿æ¨¡å— - ç´¢å¼•æ„å»º+æ ·æœ¬æ”¶é›†+RAGåŠŸèƒ½
è´Ÿè´£å€’æ’ç´¢å¼•æ„å»ºã€æ–‡æ¡£ç®¡ç†ã€æ ·æœ¬æ”¶é›†ã€RAGæ£€ç´¢å¢å¼ºç­‰ä»»åŠ¡"""

import json
import math
import os
import re
from collections import defaultdict, Counter
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Tuple, Optional

import jieba
import pandas as pd


@dataclass
class RAGResult:
    """RAGç»“æœæ•°æ®ç±»"""
    query: str
    retrieved_docs: List[Tuple[str, float, str]]
    context: str
    answer: str
    metadata: Dict


class InvertedIndex:
    """å€’æ’ç´¢å¼•ç±»"""

    def __init__(self):
        self.index = defaultdict(set)  # è¯é¡¹ -> æ–‡æ¡£IDé›†åˆ
        self.doc_lengths = {}  # æ–‡æ¡£ID -> æ–‡æ¡£é•¿åº¦
        self.documents = {}  # æ–‡æ¡£ID -> æ–‡æ¡£å†…å®¹
        self.term_freq = defaultdict(dict)  # è¯é¡¹ -> {æ–‡æ¡£ID: è¯é¢‘}
        self.doc_freq = defaultdict(int)  # è¯é¡¹ -> æ–‡æ¡£é¢‘ç‡
        self.doc_vectors = {}  # æ–‡æ¡£ID -> TF-IDFå‘é‡ï¼ˆç¨€ç–è¡¨ç¤ºï¼‰
        # åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°',
            'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'
            }

    def preprocess_text(self, text: str) -> List[str]:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # åˆ†è¯
        words = jieba.lcut(text.lower())
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        words = [word for word in words if len(word) > 1 and word not in self.stop_words]
        return words

    def add_document(self, doc_id: str, content: str):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        # ä¿å­˜åŸå§‹æ–‡æ¡£
        self.documents[doc_id] = content
        # é¢„å¤„ç†æ–‡æœ¬
        words = self.preprocess_text(content)
        # è®¡ç®—æ–‡æ¡£é•¿åº¦
        self.doc_lengths[doc_id] = len(words)
        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(words)
        # æ›´æ–°å€’æ’ç´¢å¼•
        for word, freq in word_freq.items():
            self.index[word].add(doc_id)
            self.term_freq[word][doc_id] = freq
        # æ›´æ–°æ–‡æ¡£é¢‘ç‡
        for word in word_freq:
            self.doc_freq[word] = len(self.index[word])

    def compute_doc_vectors(self):
        """è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„TF-IDFå‘é‡"""
        total_docs = len(self.documents)
        for doc_id in self.documents:
            doc_vector = {}
            for word in self.index:
                if doc_id in self.index[word]:
                    tf = self.term_freq[word][doc_id] / self.doc_lengths[doc_id]
                    idf = math.log(total_docs / self.doc_freq[word])
                    doc_vector[word] = tf * idf
            self.doc_vectors[doc_id] = doc_vector

    def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£ä»ç´¢å¼•"""
        if doc_id not in self.documents:
            return False
        # è·å–æ–‡æ¡£çš„è¯é¢‘ä¿¡æ¯
        content = self.documents[doc_id]
        words = self.preprocess_text(content)
        word_freq = Counter(words)
        # ä»å€’æ’ç´¢å¼•ä¸­ç§»é™¤æ–‡æ¡£
        for word in word_freq:
            if word in self.index:
                self.index[word].discard(doc_id)
                # å¦‚æœè¯é¡¹æ²¡æœ‰æ–‡æ¡£äº†ï¼Œåˆ é™¤è¯¥è¯é¡¹
                if not self.index[word]:
                    del self.index[word]
                    if word in self.term_freq:
                        del self.term_freq[word]
                    if word in self.doc_freq:
                        del self.doc_freq[word]
                else:
                    # æ›´æ–°è¯é¢‘ä¿¡æ¯
                    if word in self.term_freq and doc_id in self.term_freq[word]:
                        del self.term_freq[word][doc_id]
                    # æ›´æ–°æ–‡æ¡£é¢‘ç‡
                    if word in self.doc_freq:
                        self.doc_freq[word] = len(self.index[word])
        # åˆ é™¤æ–‡æ¡£ç›¸å…³æ•°æ®
        del self.documents[doc_id]
        if doc_id in self.doc_lengths:
            del self.doc_lengths[doc_id]
        if doc_id in self.doc_vectors:
            del self.doc_vectors[doc_id]
        return True

    def search(self, query: str, top_k: int = 5, min_score: float = 0.0) -> List[Tuple[str, float, str]]:
        """æœç´¢æ–‡æ¡£"""
        # é¢„å¤„ç†æŸ¥è¯¢
        query_words = self.preprocess_text(query)
        if not query_words:
            return []
        # è®¡ç®—TF-IDFåˆ†æ•°
        scores = {}
        total_docs = len(self.documents)
        for doc_id in self.documents:
            score = 0
            for word in query_words:
                if word in self.index and doc_id in self.index[word]:
                    # TF
                    tf = self.term_freq[word][doc_id] / self.doc_lengths[doc_id]
                    # IDF
                    idf = math.log(total_docs / self.doc_freq[word])
                    # TF-IDF
                    score += tf * idf
            if score > min_score:
                scores[doc_id] = score
        # æ’åºå¹¶è¿”å›ç»“æœ
        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        # ç”Ÿæˆæ‘˜è¦
        results = []
        for doc_id, score in sorted_results[:top_k]:
            summary = self.generate_summary(doc_id, query_words)
            results.append((doc_id, score, summary))
        return results

    def generate_summary(self, doc_id: str, query_words: List[str], max_length: int = 200) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        content = self.documents[doc_id]
        # æ‰¾åˆ°åŒ…å«æœ€å¤šæŸ¥è¯¢è¯çš„æ–‡æœ¬çª—å£
        best_window = ""
        best_score = 0
        # ç®€å•çš„æ»‘åŠ¨çª—å£æ–¹æ³•
        words = content.split()
        for i in range(len(words)):
            for j in range(i + 1, min(i + 50, len(words) + 1)):  # æœ€å¤š50ä¸ªè¯
                window = " ".join(words[i:j])
                window_words = self.preprocess_text(window)
                # è®¡ç®—çª—å£åŒ…å«çš„æŸ¥è¯¢è¯æ•°é‡
                score = sum(1 for word in query_words if word in window_words)
                if score > best_score and len(window) <= max_length:
                    best_score = score
                    best_window = window
        if not best_window:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„çª—å£ï¼Œä½¿ç”¨æ–‡æ¡£å¼€å¤´
            best_window = content[:max_length]
        # é«˜äº®æŸ¥è¯¢è¯
        highlighted_summary = self.highlight_keywords(best_window, query_words)
        return highlighted_summary

    def highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """é«˜äº®å…³é”®è¯"""
        highlighted_text = text
        for keyword in keywords:
            if keyword in highlighted_text:
                highlighted_text = highlighted_text.replace(
                    keyword,
                    f'<span style="background-color: yellow; font-weight: bold;">{keyword}</span>'
                    )
        return highlighted_text

    def get_document(self, doc_id: str) -> str:
        """è·å–æ–‡æ¡£å†…å®¹"""
        return self.documents.get(doc_id, "")

    def get_all_documents(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰æ–‡æ¡£"""
        return self.documents.copy()

    def get_index_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        total_documents = len(self.documents)
        total_terms = len(self.index)
        if total_documents > 0:
            average_doc_length = sum(self.doc_lengths.values()) / total_documents
        else:
            average_doc_length = 0
        return {
            'total_documents': total_documents,
            'total_terms': total_terms,
            'average_doc_length': average_doc_length
            }

    def save_to_file(self, filename: str):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        data = {
            'index': {k: list(v) for k, v in self.index.items()},
            'doc_lengths': self.doc_lengths,
            'documents': self.documents,
            'term_freq': {k: dict(v) for k, v in self.term_freq.items()},
            'doc_freq': dict(self.doc_freq)
            }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {filename}")

    def load_from_file(self, filename: str):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.index = defaultdict(set)
        for k, v in data['index'].items():
            self.index[k] = set(v)
        self.doc_lengths = data['doc_lengths']
        self.documents = data['documents']
        self.term_freq = defaultdict(dict)
        for k, v in data['term_freq'].items():
            self.term_freq[k] = v
        self.doc_freq = defaultdict(int)
        for k, v in data['doc_freq'].items():
            self.doc_freq[k] = v
        print(f"âœ… ç´¢å¼•å·²ä»æ–‡ä»¶åŠ è½½: {filename}")


class RAGEngine:
    """RAGå¼•æ“ - æ£€ç´¢å¢å¼ºç”Ÿæˆ"""

    def __init__(self, index: InvertedIndex):
        self.index = index
        self.templates = {
            'default': """åŸºäºä»¥ä¸‹ç›¸å…³æ–‡æ¡£å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š""",
            'summary': """è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹æ€»ç»“å…³äº"{query}"çš„ä¿¡æ¯ï¼š

{context}

æ€»ç»“ï¼š""",
            'comparison': """è¯·æ¯”è¾ƒä»¥ä¸‹æ–‡æ¡£ä¸­å…³äº"{query}"çš„ä¸åŒè§‚ç‚¹æˆ–æ–¹é¢ï¼š

{context}

æ¯”è¾ƒåˆ†æï¼š"""
            }

    def retrieve_context(self, query: str, top_k: int = 3, max_context_length: int = 1000) -> Tuple[
        str, List[Tuple[str, float, str]]]:
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        # ä½¿ç”¨TF-IDFæœç´¢ç›¸å…³æ–‡æ¡£
        results = self.index.search(query, top_k=top_k)

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for doc_id, score, summary in results:
            doc_content = self.index.get_document(doc_id)
            context_parts.append(f"[æ–‡æ¡£ {doc_id} (ç›¸å…³åº¦: {score:.3f})]:\n{doc_content.strip()}\n")

        context = "\n---\n".join(context_parts)

        # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        if len(context) > max_context_length:
            context = context[:max_context_length] + "..."

        return context, results

    def generate_answer(self, query: str, context: str, template_type: str = 'default') -> str:
        """ç”Ÿæˆç­”æ¡ˆï¼ˆè¿™é‡Œä½¿ç”¨ç®€å•çš„æ¨¡æ¿å’Œè§„åˆ™ï¼‰"""
        # é€‰æ‹©æ¨¡æ¿
        template = self.templates.get(template_type, self.templates['default'])
        prompt = template.format(query=query, context=context)

        # è¿™é‡Œå¯ä»¥é›†æˆçœŸå®çš„LLMï¼Œç°åœ¨ä½¿ç”¨ç®€å•çš„è§„åˆ™ç”Ÿæˆ
        answer = self._simple_answer_generation(query, context)

        return answer

    def _simple_answer_generation(self, query: str, context: str) -> str:
        """ç®€å•çš„ç­”æ¡ˆç”Ÿæˆï¼ˆåŸºäºè§„åˆ™å’Œæ¨¡æ¿ï¼‰"""
        query_words = self.index.preprocess_text(query)

        # æå–ç›¸å…³å¥å­
        sentences = [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ]', context) if s.strip()]
        relevant_sentences = []

        for sent in sentences:
            sent_words = self.index.preprocess_text(sent)
            # è®¡ç®—å¥å­ä¸æŸ¥è¯¢çš„ç›¸å…³åº¦
            relevance = sum(1 for word in query_words if word in sent_words)
            if relevance > 0:
                relevant_sentences.append((relevance, sent))

        # æŒ‰ç›¸å…³åº¦æ’åº
        relevant_sentences.sort(key=lambda x: x[0], reverse=True)

        # ç”Ÿæˆç­”æ¡ˆ
        if relevant_sentences:
            # å–æœ€ç›¸å…³çš„2-3ä¸ªå¥å­
            top_sentences = [sent for _, sent in relevant_sentences[:3]]
            answer = "æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œ" + "".join(top_sentences)

            # æ·»åŠ æ€»ç»“
            if len(relevant_sentences) > 3:
                answer += f"\n\næ­¤å¤–ï¼Œæ–‡æ¡£ä¸­è¿˜åŒ…å«äº†å…¶ä»–{len(relevant_sentences) - 3}æ¡ç›¸å…³ä¿¡æ¯ã€‚"
        else:
            answer = f"æŠ±æ­‰ï¼Œåœ¨ç°æœ‰æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸'{query}'ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯è¿›è¡Œæœç´¢ã€‚"

        return answer

    def query(
            self,
            query: str,
            top_k: int = 3,
            template_type: str = 'default',
            include_sources: bool = True
            ) -> RAGResult:
        """æ‰§è¡ŒRAGæŸ¥è¯¢"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context, retrieved_docs = self.retrieve_context(query, top_k=top_k)

        # 2. ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(query, context, template_type)

        # 3. æ·»åŠ æ¥æºä¿¡æ¯
        if include_sources and retrieved_docs:
            sources = "\n\nä¿¡æ¯æ¥æºï¼š\n"
            for doc_id, score, _ in retrieved_docs:
                sources += f"- {doc_id} (ç›¸å…³åº¦: {score:.3f})\n"
            answer += sources

        # 4. æ„å»ºç»“æœ
        result = RAGResult(
            query=query,
            retrieved_docs=retrieved_docs,
            context=context,
            answer=answer,
            metadata={
                'template_type': template_type,
                'top_k': top_k,
                'timestamp': datetime.now().isoformat()
                }
            )

        return result

    def batch_query(self, queries: List[str], **kwargs) -> List[RAGResult]:
        """æ‰¹é‡æŸ¥è¯¢"""
        results = []
        for query in queries:
            result = self.query(query, **kwargs)
            results.append(result)
        return results


class SampleCollector:
    """æ ·æœ¬æ”¶é›†å™¨"""

    def __init__(self):
        self.samples = []
        self.rag_samples = []  # RAGæŸ¥è¯¢æ ·æœ¬

    def add_sample(self, sample: Dict):
        """æ·»åŠ æ ·æœ¬"""
        sample['timestamp'] = datetime.now().isoformat()
        self.samples.append(sample)

    def add_rag_sample(self, rag_result: RAGResult, user_feedback: Optional[Dict] = None):
        """æ·»åŠ RAGæ ·æœ¬"""
        rag_sample = {
            'query': rag_result.query,
            'retrieved_docs': [(doc_id, score) for doc_id, score, _ in rag_result.retrieved_docs],
            'answer': rag_result.answer,
            'metadata': rag_result.metadata,
            'timestamp': datetime.now().isoformat()
            }
        if user_feedback:
            rag_sample['feedback'] = user_feedback
        self.rag_samples.append(rag_sample)

    def get_samples(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ ·æœ¬"""
        return self.samples

    def get_rag_samples(self) -> List[Dict]:
        """è·å–RAGæ ·æœ¬"""
        return self.rag_samples

    def export_samples(self, filename: str):
        """å¯¼å‡ºæ ·æœ¬åˆ°æ–‡ä»¶"""
        # å¯¼å‡ºæ™®é€šæ ·æœ¬
        if self.samples:
            df = pd.DataFrame(self.samples)
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"âœ… æ ·æœ¬å·²å¯¼å‡ºåˆ°: {filename}")

        # å¯¼å‡ºRAGæ ·æœ¬
        if self.rag_samples:
            rag_filename = filename.replace('.csv', '_rag.csv')
            df_rag = pd.DataFrame(self.rag_samples)
            df_rag.to_csv(rag_filename, index=False, encoding='utf-8')
            print(f"âœ… RAGæ ·æœ¬å·²å¯¼å‡ºåˆ°: {rag_filename}")

    def get_stats(self) -> Dict:
        """è·å–æ ·æœ¬ç»Ÿè®¡"""
        stats = {
            'total_samples': len(self.samples),
            'total_rag_samples': len(self.rag_samples),
            }

        if self.samples:
            total_clicks = sum(sample.get('clicked', 0) for sample in self.samples)
            stats['click_rate'] = total_clicks / len(self.samples) if len(self.samples) > 0 else 0

        if self.rag_samples:
            # RAGæ ·æœ¬ç»Ÿè®¡
            total_positive_feedback = sum(
                1 for sample in self.rag_samples
                if sample.get('feedback', {}).get('helpful', False)
                )
            stats['rag_helpful_rate'] = total_positive_feedback / len(self.rag_samples) if len(
                self.rag_samples
                ) > 0 else 0

        return stats


def create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    documents = {
        "doc1": """
        äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
        è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
        """,
        "doc2": """
        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨ç»Ÿè®¡å­¦æ–¹æ³•è®©è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿ"å­¦ä¹ "ï¼ˆå³ï¼Œé€æ­¥æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼‰ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
        æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†ææ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚
        """,
        "doc3": """
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œã€‚
        æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥è‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡è¡¨ç¤ºï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚
        """,
        "doc4": """
        è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦çš„ä¸€ä¸ªäº¤å‰é¢†åŸŸï¼Œå®ƒç ”ç©¶è®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚
        NLPæŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨äºæœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿå’ŒèŠå¤©æœºå™¨äººç­‰åº”ç”¨ã€‚
        """,
        "doc5": """
        è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å¾—é«˜å±‚æ¬¡çš„ç†è§£ã€‚
        è®¡ç®—æœºè§†è§‰æŠ€æœ¯åŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²å’Œè§†é¢‘åˆ†æç­‰ã€‚
        """,
        "doc6": """
        ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿ç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„è®¡ç®—æ¨¡å‹ï¼Œç”±å¤§é‡ç›¸äº’è¿æ¥çš„ç¥ç»å…ƒç»„æˆã€‚
        ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œåœ¨æ¨¡å¼è¯†åˆ«å’Œé¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚
        """,
        "doc7": """
        å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è®©æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚
        å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆã€æœºå™¨äººæ§åˆ¶å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚
        """,
        "doc8": """
        çŸ¥è¯†å›¾è°±æ˜¯ä¸€ç§ç»“æ„åŒ–çš„çŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ï¼Œå°†å®ä½“å’Œå…³ç³»ç»„ç»‡æˆå›¾ç»“æ„ã€‚
        çŸ¥è¯†å›¾è°±åœ¨æœç´¢å¼•æ“ã€æ¨èç³»ç»Ÿå’Œé—®ç­”ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚
        """,
        "doc9": """
        å¤§æ•°æ®æ˜¯æŒ‡æ— æ³•ç”¨ä¼ ç»Ÿæ•°æ®å¤„ç†è½¯ä»¶åœ¨åˆç†æ—¶é—´å†…å¤„ç†çš„æ•°æ®é›†ã€‚
        å¤§æ•°æ®æŠ€æœ¯åŒ…æ‹¬æ•°æ®å­˜å‚¨ã€æ•°æ®å¤„ç†ã€æ•°æ®åˆ†æå’Œæ•°æ®å¯è§†åŒ–ç­‰æ–¹é¢ã€‚
        """,
        "doc10": """
        äº‘è®¡ç®—æ˜¯ä¸€ç§é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºçš„æœåŠ¡æ¨¡å¼ã€‚
        äº‘è®¡ç®—åŒ…æ‹¬åŸºç¡€è®¾æ–½å³æœåŠ¡ã€å¹³å°å³æœåŠ¡å’Œè½¯ä»¶å³æœåŠ¡ç­‰ä¸åŒå±‚æ¬¡ã€‚
        """
        }
    return documents


def build_index_from_documents(documents: Dict[str, str], save_path: str = ""):
    """ä»æ–‡æ¡£æ„å»ºç´¢å¼•"""
    print("ğŸ”¨ æ„å»ºå€’æ’ç´¢å¼•...")
    index = InvertedIndex()
    for doc_id, content in documents.items():
        index.add_document(doc_id, content)
        print(f"   æ·»åŠ æ–‡æ¡£: {doc_id}")

    # è®¡ç®—æ–‡æ¡£å‘é‡
    index.compute_doc_vectors()

    stats = index.get_index_stats()
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ:")
    print(f"   æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"   æ€»è¯é¡¹æ•°: {stats['total_terms']}")
    print(f"   å¹³å‡æ–‡æ¡£é•¿åº¦: {stats['average_doc_length']:.2f}")
    if save_path:
        index.save_to_file(save_path)
    return index


def test_rag_functionality(index: InvertedIndex):
    """æµ‹è¯•RAGåŠŸèƒ½"""
    print("\nğŸ¤– æµ‹è¯•RAGåŠŸèƒ½:")
    print("=" * 50)

    # åˆ›å»ºRAGå¼•æ“
    rag_engine = RAGEngine(index)

    # åˆ›å»ºæ ·æœ¬æ”¶é›†å™¨
    collector = SampleCollector()

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "è‡ªç„¶è¯­è¨€å¤„ç†æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
        "è®¡ç®—æœºè§†è§‰åŒ…æ‹¬å“ªäº›æŠ€æœ¯ï¼Ÿ",
        "å¤§æ•°æ®å’Œäº‘è®¡ç®—çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]

    for query in test_queries:
        print(f"\nğŸ“ æŸ¥è¯¢: {query}")
        print("-" * 40)

        # æ‰§è¡ŒRAGæŸ¥è¯¢
        result = rag_engine.query(query, top_k=3)

        print(f"æ£€ç´¢åˆ° {len(result.retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£:")
        for doc_id, score, _ in result.retrieved_docs:
            print(f"  - {doc_id}: ç›¸å…³åº¦ {score:.3f}")

        print(f"\nğŸ’¡ ç”Ÿæˆçš„ç­”æ¡ˆ:")
        print(result.answer)

        # æ”¶é›†æ ·æœ¬
        collector.add_rag_sample(result, user_feedback={'helpful': True})

    # æµ‹è¯•ä¸åŒçš„æ¨¡æ¿ç±»å‹
    print("\n\nğŸ“Š æµ‹è¯•ä¸åŒæ¨¡æ¿ç±»å‹:")
    print("=" * 50)

    query = "äººå·¥æ™ºèƒ½çš„å„ä¸ªåˆ†æ”¯"
    for template_type in ['summary', 'comparison']:
        print(f"\nä½¿ç”¨ {template_type} æ¨¡æ¿:")
        result = rag_engine.query(query, template_type=template_type)
        print(result.answer)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = collector.get_stats()
    print("\n\nğŸ“ˆ æ ·æœ¬ç»Ÿè®¡:")
    print(f"  - RAGæŸ¥è¯¢æ ·æœ¬æ•°: {stats['total_rag_samples']}")
    print(f"  - æœ‰ç”¨ç‡: {stats.get('rag_helpful_rate', 0):.2%}")

    return collector


def main():
    """ä¸»å‡½æ•° - æ„å»ºç¤ºä¾‹ç´¢å¼•å¹¶æµ‹è¯•RAGåŠŸèƒ½"""
    print("ğŸ—ï¸  ç¦»çº¿ç´¢å¼•æ„å»ºæ¨¡å— + RAGåŠŸèƒ½")
    print("=" * 50)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('models', exist_ok=True)
    os.makedirs('samples', exist_ok=True)

    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    documents = create_sample_documents()

    # æ„å»ºç´¢å¼•
    index = build_index_from_documents(documents, 'models/index_data.json')

    # æµ‹è¯•åŸºç¡€æœç´¢
    print("\n\nğŸ” æµ‹è¯•åŸºç¡€TF-IDFæœç´¢:")
    print("=" * 50)
    test_queries = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = index.search(query, top_k=3)
        for doc_id, score, summary in results:
            print(f"  - {doc_id}: {score:.4f}")

    # æµ‹è¯•RAGåŠŸèƒ½
    collector = test_rag_functionality(index)

    # å¯¼å‡ºæ ·æœ¬
    if collector.rag_samples:
        collector.export_samples('samples/search_samples.csv')

    print("\n\nâœ… ç¦»çº¿ç´¢å¼•æ„å»ºå’ŒRAGåŠŸèƒ½æµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ ç°åœ¨å¯ä»¥å¯åŠ¨åœ¨çº¿æœåŠ¡: python online_service.py")
    print("ğŸ“š RAGåŠŸèƒ½å·²é›†æˆï¼Œå¯ä»¥æä¾›æ›´æ™ºèƒ½çš„æœç´¢ç­”æ¡ˆ")


if __name__ == "__main__":
    main()
